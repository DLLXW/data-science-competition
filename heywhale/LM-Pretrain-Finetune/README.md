## GPT-2/Bart/CPTçš„é¢„è®­ç»ƒå’Œå¾®è°ƒå…¨æµç¨‹è®­ç»ƒ
[æ•°æ®é›†åœ°å€](https://www.heywhale.com/org/gaiic2023/competition/area/63fef766b4422ee27402289d/leaderboard)
æ•°æ®æ¥è‡ªäºä¸€ä¸ªè„±æ•çš„æ¯”èµ›æ•°æ®é›†ï¼šè„±æ•åçš„å½±åƒæè¿°ä¸å¯¹åº”å½±åƒæŠ¥å‘Šã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡å½±åƒæè¿°æ¥è‡ªåŠ¨ç”Ÿæˆå½±å“æŠ¥å‘Šã€‚
|åˆ—å|	ç±»å‹	|ç¤ºä¾‹|
|  ----  | ---- | ----  |
|report_ID	|int	|1|
|description|	è„±æ•åçš„å½±åƒæè¿°ï¼Œä»¥å­—ä¸ºå•ä½ä½¿ç”¨ç©ºæ ¼åˆ†å‰²|	101 47 12 66 74 90 0 411 234 79 175|
|diagnosis	|è„±æ•åçš„è¯Šæ–­æŠ¥å‘Šï¼Œä»¥å­—ä¸ºå•ä½ä½¿ç”¨ç©ºæ ¼åˆ†å‰²	|122 83 65 74 2 232 18 44 95|
## 1.Bart/CPT Pretrain
**åŸºäºDeepSpeed-Megatroné¢„è®­ç»ƒBartå’ŒCPTæ¨¡å‹**
### 1.1 æ•°æ®ç›¸å…³
é¢„è®­ç»ƒæ•°æ®éµå¾ªmegatronçš„æ ‡å‡†å¤„ç†èŒƒå¼ï¼Œä½†æ˜¯å› ä¸ºè¿™é‡Œæ˜¯å·²ç»è„±æ•çš„æ•°æ®ï¼Œæ²¡æ³•ç”¨ç°æˆçš„åˆ†è¯å™¨è¿›è¡Œåˆ†è¯è½¬idã€‚æ‰€ä»¥è¿™é‡Œç›´æ¥è·³è¿‡åˆ†è¯è¿™ä¸€æ­¥ã€‚
```bash
cd pretrain
#ä¸ºäº†ç¬¦åˆmegatronæ ‡å‡†æ ¼å¼ï¼Œå…ˆå°†åŸå§‹æ•°æ®è½¬jsonlæ ¼å¼
python convert_to_json.py
#
python tools/preprocess_data.py \ 
       --input diag_train.json \ #å¤„ç†å¥½çš„jsonæ ¼å¼çš„æ•°æ®
       --output-prefix desc_diag \ #è¾“å‡ºæ–‡ä»¶çš„å‰ç¼€ï¼ˆéšä¾¿å†™ï¼‰
       --vocab vocab/vocab.txt \ #è¯è¡¨ï¼Œå› ä¸ºæˆ‘ä»¬æ— éœ€åˆ†ï¼Œæ‰€ä»¥ä»£ç ä¸­ä¸ä¼šå®é™…ç”¨åˆ°ï¼Œè¿™é‡Œéšä¾¿å†™ï¼ˆæˆ‘è¿™é‡Œé€‰æ‹©ç›´æ¥ä½¿ç”¨Bertçš„è¯è¡¨,å»Huggingfaceä¸‹è½½ï¼‰
       --dataset-impl mmap \ 
       --tokenizer-type Huggingface \
       --split-sentences
```
ä¸éœ€è¦åˆ†è¯æ‰€ä»¥å¯¹./tools/preprocess_data.pyé‡Œé¢139-141è¡Œè¿›è¡Œäº†æ›´æ”¹ï¼Œå› ä¸ºæ•°æ®é›†ä¸­æœ€å°idæ˜¯9ï¼Œæœ€å¤§1300ï¼Œè¿™é‡Œé€‰æ‹©å¯¹æ‰€æœ‰id+100ï¼Œè¿™ä¹ˆåšçš„ç›®çš„
#åœ¨äºé¢„è®­ç»ƒè¿‡ç¨‹ä¸­åˆ©ç”¨äº†Bertçš„ç°æˆvocab.txtï¼Œè€ŒBertçš„è¯è¡¨é‡Œé¢[CLS][SEP][MASK]åˆ†åˆ«å¯¹åº”çš„idä¸º101,102,103ã€‚
#å¯¹æ•°æ®é›†+100åˆšå¥½è·³è¿‡ã€‚å½“ç„¶å…¶å®å¯ä»¥ä¸åšï¼Œè‡ªå·±åœ¨é¢„è®­ç»ƒçš„é…ç½®æ–‡ä»¶é‡Œé¢æŒ‡å®šè¿™å‡ ä¸ªç‰¹æ®Šç¬¦å·ä¹Ÿè¡Œã€‚

### 1.2é¢„è®­ç»ƒç›¸å…³

é…ç½®æ–‡ä»¶ï¼ˆconfig.jsonï¼‰ä»¥åŠè¯è¡¨ï¼ˆvocab.txtï¼‰ä¸‹è½½ï¼š
- [fnlp/bart-large-chinese](https://huggingface.co/fnlp/bart-large-chinese)æ”¾åˆ°vocab-bart-large/
- [fnlp/cpt-large](https://huggingface.co/fnlp/cpt-large)æ”¾åˆ°vocab-cpt-large/

**âš ï¸æ³¨æ„ï¼š** è¿™é‡Œè§£é‡Šä¸€ä¸‹è¯è¡¨çš„ä½œç”¨ï¼Œè™½ç„¶æˆ‘ä»¬ä¸éœ€è¦åˆ†è¯ï¼Œ**ä½†æ˜¯ä¸ºäº†æ›´å°‘çš„æ”¹åŠ¨ä»£ç **ï¼Œé€‰æ‹©å°†è¯è¡¨æ”¾åˆ°ä¸‹é¢ã€‚ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è¦è¯è¡¨ï¼Œé‚£ä¹ˆéœ€è¦è‡ªä¸ªå»æ”¹ä¸€æ”¹./megatron/data/bart_dataset.pyä¸‹é¢çš„å„ç§ç‰¹æ®Šç¬¦å·ã€‚ï¼ˆå¦å¤–ï¼šé™¤äº†é‚£äº›ç‰¹æ®Šç¬¦å·ï¼Œ[CLS][SEP][MASK]ï¼Œå…¶ä½™çš„è¯å¯ä»¥åˆ æ‰ï¼Œå¯ä»¥å‡å°ä¸€ç‚¹æ¨¡å‹çš„å‚æ•°é‡å’Œè®¡ç®—é‡ï¼‰
ä¸Šé¢ğŸ‘†ä¿©éƒ½é’ˆå¯¹largeæ¨¡å‹çš„é…ç½®ã€‚
```bash
#é¢„è®­ç»ƒBartæ¨¡å‹
./run_pretrain_bart.sh
#é¢„è®­ç»ƒCPTæ¨¡å‹
./run_pretrain_cpt.sh
```
### 1.3 æºç ç»†èŠ‚

ä»¥Bartæ¨¡å‹ä¸ºä¾‹ï¼š
####  **./megatron/model/bart_model.py ** 
æ”¹çš„ä¸»è¦æ˜¯__init__(self): æ³¨é‡Šæ‰æ§åˆ¶æ¨¡å‹å¤§å°ï¼ˆbase/largeï¼‰çš„å‚æ•°ï¼Œä¸€åˆ‡ä»¥config.jsonä¸ºå‡†ã€‚åŒæ—¶é‡‡å–å¢é‡é¢„è®­ç»ƒï¼Œä¹Ÿå³ä¸ºè¯»å–å®˜æ–¹é¢„è®­ç»ƒå¥½çš„æƒé‡ç”¨æœ¬æ•°æ®é›†å¢é‡é¢„è®­ç»ƒã€‚è‹¥æ”¹äº†è¯è¡¨å¤§å°ï¼Œåˆ™è®°å¾—popæ‰æ¨¡å‹æƒé‡é‡Œé¢é‚£äº›ä¸åŒ¹é…çš„å±‚ã€‚

#### **./megatron/data/bart_dataset.py**

ï¼ˆ1ï¼‰58è¡Œï¼šæ ‡ç‚¹ç¬¦å·åˆ†å‰²ç¬¦åˆ¤å®šï¼Œå› ä¸ºæœ¬æ•°æ®é›†æ— æ³•ç¡®å®šæ ‡ç‚¹ç¬¦å·çš„idï¼Œï¼ˆå…¶å®ä¹Ÿå¯ä»¥çœ‹å‡ºæ¥å“ªäº›æ˜¯æ ‡ç‚¹ç¬¦å·ï¼‰ï¼Œæ‰€ä»¥è¿™é‡Œå¹²è„†ç½®ç©ºã€‚è¿™æ ·ä¼šå¯¼è‡´çš„å½±å“æ˜¯åœ¨è¿›è¡Œå»å™ªè‡ªç¼–ç ï¼ˆDenoising Autoencoderï¼ŒDAEï¼‰é¢„è®­ç»ƒçš„æ—¶å€™ä¼šå°‘ä¸€ç§å¥å­é‡æ’çš„ç ´åè§„åˆ™ã€‚

ï¼ˆ2ï¼‰282-303è¡Œï¼šword_starts(self, source)æ–¹æ³•ã€‚ä½œè€…ä¹Ÿåˆšå…¥é—¨LMï¼Œæ ¹æ®å¯¹æºç çš„é˜…è¯»ï¼Œæ²¡ç†è§£é”™çš„è¯è¯¥æ–¹æ³•ä¸»è¦æ˜¯ä¸ºäº†ç¡®å®šè¯ç»„çš„ä½ç½®ï¼Œç„¶åæ ¹æ®è¿™äº›ä½ç½®è¿›è¡ŒDAEçš„å„ç§ç ´ååŸæ–‡æœ¬çš„æ“ä½œã€‚å› ä¸ºæœ¬æ•°æ®é›†ç»è¿‡äº†è„±æ•ï¼Œæ‰€ä»¥æ— æ³•å‡†ç¡®ç¡®å®šè¿™äº›ä½ç½®ï¼ˆå¦‚æœæ²¡è„±æ•ï¼Œæºç é‡Œé¢å°±å¯ä»¥ç”¨jiebaåˆ†è¯ç¡®å®šäº†ï¼‰ï¼Œæ‰€ä»¥è¿™é‡Œæˆ‘é‡‡å–äº†ä¸€ä¸ªæœ€ç®€å•çš„æ–¹æ¡ˆæ˜¯ï¼Œéšæœºçš„äº§ç”Ÿä¸€äº›â€œword start positionâ€ã€‚

### 1.4é¢„è®­ç»ƒå¥½çš„æ¨¡å‹å¦‚ä½•ä½¿ç”¨
```python
weight=torch.load('./checkpoints/bart-large/iter_0004000/mp_rank_00/model_optim_rng.pt')
torch.save(weight['model']['language_model'],'./custom_pretrain_bart/pytorch_model.bin')
#config.jsonä¹Ÿè¦æ”¾å…¥custom_pretrain_bart/
from transformers import BartForConditionalGeneration
model = BartForConditionalGeneration.from_pretrained("custom_pretrain_bart/")
```

### 1.5é¢„è®­ç»ƒç¯å¢ƒ
å€¼å¾—ä¸€æçš„æ˜¯éœ€è¦apexåº“ï¼Œæ‰€ä»¥éœ€è¦
```bash
git clone https://github.com/NVIDIA/apex
cd apex
pip install -v --disable-pip-version-check --no-cache-dir --global-option="--cpp_ext" --global-option="--cuda_ext" ./
```
å…¶ä½™çš„åŒ…å‚è€ƒrequirements.txt

### 2.GPT-2çš„é¢„è®­ç»ƒ
å› ä¸ºæœ€è¿‘LLMè¿™ä¹ˆç«ï¼Œä¸ºäº†æ›´å¥½çš„ç†è§£GPTçš„åŸç†ï¼Œè¿™é‡Œå°†GPT-2çš„æœ€ç²¾ç®€çš„ä»£ç æ‹¿æ¥ç›˜ä¸€ç›˜ã€‚ä»£ç è¿‡äºç®€å•ï¼Œåªæœ‰ä¸¤ä¸‰ä¸ªæ–‡ä»¶ï¼Œå»ºè®®å¥½å¥½é˜…è¯»æºç ã€‚ä»£ç è§/GPT-2ï¼Œä¸€çœ‹å°±æ‡‚ã€‚ä¸å†èµ˜è¿°ã€‚

**ç®€å•è¯´å‡ ç‚¹ï¼š**
- å°†æè¿°å’Œè¯Šæ–­æ•°æ®æ‹¼æ¥èµ·æ¥ï¼Œä¸­é—´åˆ†éš”ç¬¦éš”å¼€ï¼Œè®°å¾—åŠ ä¸Šç»“æŸç¬¦å·ï¼Œè¿™å°±æ˜¯ä¸€æ¡æ ·æœ¬ã€‚
- åºåˆ—é•¿åº¦è®¾ç½®ä¸º256ï¼Œä¸å¤Ÿçš„è¿›è¡Œpadding
- è¾“å…¥xè¿›è¡Œä½ç§»æ“ä½œx[1:]å°±æ˜¯è¿™æ¡æ ·æœ¬å¯¹åº”çš„label
- CausalSelfAttentionæ‰§è¡Œnext wordé¢„æµ‹

å¯¹å°ç™½çš„âš ï¸å»ºè®®ï¼šæŠŠGPT-2/model.pyé‡Œé¢çš„å®ç°é€è¡Œç²¾è¯»

## 3.finetune
train pipelineæ•´ä½“ä»£ç å¾ˆç®€å•

æ ¸å¿ƒåœ¨äºæ„é€ ï¼š
- input_ids
- attention_mask
- decoder_input_ids
- labels

å¼€å§‹ç¬¦å·ï¼Œç»“æŸç¬¦å·ï¼ŒPadç­‰ç¬¦å·å¾ˆé‡è¦ï¼Œä¸è¦ä¹±å¡«ã€‚è¯¦è§dataset.py
```python
class DiagDataset(Dataset):
    def __init__(self,df,max_length=128,finetune=False):
        super().__init__()
        self.df = df.reset_index(drop=True).copy()
        self.max_length = max_length
        self.finetune = finetune
    def __len__(self):
        return self.df.shape[0]
    def __getitem__(self, index: int):
        #ä¸‹é¢çš„ä»£ç æ˜¯finetuneæ•°æ®å‡†å¤‡çš„æ ¸å¿ƒ
        sample = self.df.iloc[index]
        desc=sample['description']
        diag=sample['diagnosis']
        desc=[101]+[int(i)+100 for i in desc.split(' ')]+[102]
        diag=[101]+[int(i)+100 for i in diag.split(' ')]+[102]
        context_len=len(desc)
        desc=desc+[0]*(self.max_length-len(desc))
        diag=diag+[0]*(self.max_length-len(diag))
        #input_id
        desc_id=np.array(desc)
        #attention mask
        desc_mask=np.array([1]*context_len+[0]*(self.max_length-context_len))
        #
        diag=np.array(diag)
        diag_id=diag[:-1].copy()
        diag_label=diag[1:].copy()
        diag_label[diag[1:]==0]=-100
        return torch.from_numpy(desc_id),torch.from_numpy(desc_mask),torch.from_numpy(diag_id),torch.from_numpy(diag_label)
```
æ‰§è¡Œè®­ç»ƒå’Œæ¨ç†å¦‚ä¸‹ï¼š
```bash
python train.py #
python infer.py #
```
## æ€»ç»“
è¯¥repoä»¥ä¸€ä¸ªæ¯”èµ›è„±æ•æ•°æ®é›†ä¸ºä¾‹ï¼Œè¯¦ç»†ä»‹ç»äº†å¦‚ä½•é’ˆå¯¹Bart/CPT/GPTç­‰æ¨¡å‹è¿›è¡Œå®Œæ•´çš„pretrain-fintune-inferenceæµç¨‹ï¼Œå¯ä»¥ä½œä¸ºå…¥é—¨LLMçš„å¾ˆå¥½å®æ“é¡¹ç›®ã€‚
ä½œè€…æœ¬äººä¹Ÿæ˜¯æœ€è¿‘ä¸€ä¸ªæ¥æœˆæ‰å¼€å§‹å­¦ä¹ é¢„è®­ç»ƒæ¨¡å‹ç›¸å…³çŸ¥è¯†ï¼Œä¸Šé¢çš„æµç¨‹ä¹Ÿå¯ä»¥çœ‹ä½œå­¦ä¹ è¿‡ç¨‹è®°å½•ï¼Œéš¾å…æœ‰ç–æ¼ä¹‹å¤„ï¼Œæ•¬è¯·è°…è§£ï¼


## å‚è€ƒ
[fastnlp/CPT](https://github.com/fastnlp/CPT)

[nanoGPT](https://github.com/karpathy/nanoGPT)